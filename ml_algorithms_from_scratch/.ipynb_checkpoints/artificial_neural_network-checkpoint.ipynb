{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function.\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Derivative of activation function.\n",
    "def d_sigmoid(x):\n",
    "    return x * (1.0 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, n_nodes_hidden, n_hidden_layers, learning_rate, n_epochs):\n",
    "        \"\"\"Class constructor.\"\"\"\n",
    "        self.n_nodes_hidden = n_nodes_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.learning_rate = np.array([learning_rate])\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def initalise_parameters(self):\n",
    "        \"\"\"Initialise the neural network parameters (weights and biases) with uniform \n",
    "        random variates in the range 0 to 1.\"\"\"\n",
    "        self.w_input = np.random.rand(self.n_nodes_input, self.n_nodes_hidden)\n",
    "        self.b_input = np.random.rand(self.n_nodes_hidden)\n",
    "\n",
    "        self.w_hidden = np.random.rand(self.n_nodes_hidden, self.n_nodes_hidden, self.n_hidden_layers - 1)\n",
    "        self.b_hidden = np.random.rand(self.n_nodes_hidden, self.n_hidden_layers - 1)\n",
    "\n",
    "        self.w_output = np.random.rand(self.n_nodes_hidden, self.n_nodes_output)\n",
    "        self.b_output = np.random.rand(self.n_nodes_output)\n",
    "        \n",
    "    def initialise_z_a_cache(self):\n",
    "        \"\"\"Initialise the z and a cache. This is necessary for updating the parameters during backpropogation.\"\"\"\n",
    "        z_hidden = np.zeros(shape = [self.n_training_samples, self.n_nodes_hidden, self.n_hidden_layers])\n",
    "        a_hidden = np.zeros(shape = [self.n_training_samples, self.n_nodes_hidden, self.n_hidden_layers])\n",
    "    \n",
    "        return z_hidden, a_hidden\n",
    "    \n",
    "    def propogate_forward(self, z_hidden, a_hidden):\n",
    "        \"\"\"Propogate information forward through the network.\"\"\"\n",
    "        z = np.dot(self.x, self.w_input) + self.b_input\n",
    "        a = sigmoid(z)\n",
    "\n",
    "        z_hidden[:, :, 0] = z\n",
    "        a_hidden[:, :, 0] = a\n",
    "\n",
    "        for i in range(0, self.n_hidden_layers - 1):\n",
    "            z = np.dot(a, self.w_hidden[:, :, i]) + self.b_hidden[:, i]\n",
    "            a = sigmoid(z)\n",
    "            z_hidden[:, :, i + 1] = z\n",
    "            a_hidden[:, :, i + 1] = a\n",
    "\n",
    "        z_output = np.dot(z, self.w_output) + self.b_output\n",
    "        a_output = sigmoid(z_output)\n",
    "\n",
    "        return z_hidden, a_hidden, z_output, a_output\n",
    "    \n",
    "    def calc_loss(self, a_output):\n",
    "        \"\"\"Calculate loss using a mean squared error loss function.\"\"\"\n",
    "        return 0.5 * np.sum((a_output.T - self.y) ** 2) / self.n_training_samples\n",
    "    \n",
    "    def propogate_backward(self, a_hidden, z_hidden, a_output, z_output):\n",
    "        \"\"\"Propogate backward through the network, updating the parameters via stochastic gradient descent.\"\"\"\n",
    "        d_z = (a_output.T - self.y) * d_sigmoid(a_output).T / self.n_training_samples\n",
    "        d_w = np.dot(d_z, a_hidden[:, :, self.n_hidden_layers - 1])\n",
    "        d_b = np.sum(d_z, axis = 1)\n",
    "\n",
    "        self.w_output -= (self.learning_rate * d_w).T\n",
    "        self.b_output -= self.learning_rate * d_b\n",
    "\n",
    "        w_next = self.w_output\n",
    "\n",
    "        for i in range(self.n_hidden_layers - 1, 0, -1):\n",
    "            d_z = np.dot(w_next, d_z) * d_sigmoid(a_hidden[:, :, i]).T / self.n_training_samples\n",
    "            d_w = np.dot(d_z, a_hidden[:, :, i - 1])\n",
    "            d_b = np.sum(d_z, axis = 1)\n",
    "\n",
    "            self.w_hidden[:, :, i - 1] -= self.learning_rate * d_w\n",
    "            self.b_hidden[:, i - 1] -= self.learning_rate * d_b\n",
    "\n",
    "            w_next = self.w_hidden[:, :, i - 1]\n",
    "\n",
    "        d_z = np.dot(w_next, d_z) * d_sigmoid(a_hidden[:, :, 0]).T / self.n_training_samples\n",
    "        d_w = np.dot(d_z, self.x)\n",
    "        d_b = np.sum(d_z, axis = 1)\n",
    "\n",
    "        self.w_input -= (self.learning_rate * d_w).T\n",
    "        self.b_input -= self.learning_rate * d_b\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        \"\"\"Train the neural network object on the training data.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        self.n_training_samples = x.shape[0]\n",
    "        self.n_nodes_input = x.shape[1]\n",
    "        \n",
    "        if (len(y.shape) == 1):\n",
    "            self.n_nodes_output = 1\n",
    "        else:\n",
    "            self.n_nodes_output = y.shape[1]\n",
    "            \n",
    "        self.initalise_parameters()\n",
    "        \n",
    "        z_hidden, a_hidden = self.initialise_z_a_cache()\n",
    "\n",
    "        for i in range(0, self.n_epochs):\n",
    "            z_hidden, a_hidden, z_output, a_output = self.propogate_forward(z_hidden, a_hidden)\n",
    "\n",
    "            self.propogate_backward(a_hidden, z_hidden, a_output, z_output)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the output y for a given input x.\"\"\"\n",
    "        z = np.dot(x, self.w_input) + self.b_input\n",
    "        a = sigmoid(z)\n",
    "        \n",
    "        for i in range(0, self.n_hidden_layers - 1):\n",
    "            z = np.dot(a, self.w_hidden[:, :, i]) + self.b_hidden[:, i]\n",
    "            a = sigmoid(z)\n",
    "\n",
    "        z_output = np.dot(z, self.w_output) + self.b_output\n",
    "        a_output = sigmoid(z_output)\n",
    "\n",
    "        return a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Activation functions to use in the neural network, and their derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x, activation_function):\n",
    "    \"\"\"Chooses\"\"\"\n",
    "    if activation_function == 'sigmoid': return sigmoid(x)\n",
    "    if activation_function == 'relu': return relu(x)\n",
    "    if activation_function == 'leaky_relu': return leaky_relu(x)\n",
    "    if activation_function == 'softplus': return softplus(x)\n",
    "    else: return 'Activation function not recognised.'\n",
    "\n",
    "def d_activation(x, activation_function):\n",
    "    \n",
    "    if activation_function == 'sigmoid': return d_sigmoid(x)\n",
    "    if activation_function == 'relu': return d_relu(x)\n",
    "    if activation_function == 'leaky_relu': return d_leaky_relu(x)\n",
    "    if activation_function == 'softplus': return d_softplus(x)\n",
    "    else: return 'Activation function not recognised.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return x * (1.0 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectified linear unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(x, 0)\n",
    "\n",
    "def d_relu(x):\n",
    "    if x >= 0: return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky rectified linear unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return max(x, 0.01 * x)\n",
    "\n",
    "def d_leaky_relu(x):\n",
    "    if x >= 0: return 1\n",
    "    else: return 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def d_softplus(x):\n",
    "    return sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Test the ANN's performance on basic logic operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(n_nodes_hidden = 2,\n",
    "                    n_hidden_layers = 2,\n",
    "                    learning_rate = 5.0,\n",
    "                    n_epochs = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train(x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n",
    "          y = np.array([0, 1, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03842953],\n",
       "       [0.96968537],\n",
       "       [0.97106383],\n",
       "       [0.03861592]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(n_nodes_hidden = 5,\n",
    "                    n_hidden_layers = 2,\n",
    "                    learning_rate = 3.0,\n",
    "                    n_epochs = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train(x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n",
    "          y = np.array([0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.35427802e-06],\n",
       "       [1.61786156e-02],\n",
       "       [1.67356623e-02],\n",
       "       [9.76561842e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
